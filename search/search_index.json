{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"borg/","text":"Borg Backup Borg provides facilitites for backing up data through efficient means through features such as deduplication and compression. Borg places all backups for a given job within a repository. These repositories then contain multiple archives (e.g. single backup). More information can be found on the docs here . Installation Installation is easy and available by default within ubuntu repositories. sudo apt-get install borgbackup Setting Up Backup There are several steps to getting a borg repository configured and enabling backups. Creating Repository Backing Keying Material Create Archives Each of the sections are discussed in greater detail. Creating Repository A Borg repository contains the backup archives that are created. In general, the repos will be encrypted with keys that are protected by a passphrase. There are 2 options for creating the repository. First, the wrapped key blob can be stored within the repo itself alongside the archives. This simplifies the restoration process as the keys are within the repository. This also means that the passphrase is only protection for data. Repos with contained keys can be created with the following borg init --encryption repokey /mnt/backup/borg The second option is called keyfile as opposed to repokey. The keyfile solution will install the wrapped keys into the server as opposed to the repo itself. This results in more secure posture as the wrapped keys and the corresponding passphrase would be required for unlocking the repos. borg init --encryption keyfile /mnt/backup/borg Keying Material Backup The keyfile option from previous step results in the keys being stored within the server. As such, it also becomes necessary to backup the keys from the server so that it can be restored should the server crash. borg key export /mnt/backup/borg Once exported, the key should be added to Bitwarden for archiving Create archives (backups) as desired Borg calls a backup an archive within their documentation. Creating an archive in Borg parlance is equivalent to running a backup. The following command can be used to create a backup of /mnt/critical into a repo located at /mnt/backup/borg . borg create \\ --verbose \\ --filter AME \\ --list \\ --progress \\ --stats \\ --show-rc \\ --compression zstd,10 \\ --exclude-caches \\ --exclude /mnt/critical/ignore \\ \\ /mnt/backup/borg::'critical-{now}' \\ /mnt/critical Recovery Process Steps 1. Install borgbackup on client 2. Download borg repository from offsite location 3. Re-create keyfile within recovery system 4. Mount Recovery drive at same path (/mnt/backup/borg) Re-create keyfile from Bitwarden The keyfile contains the decryption/MAC keys for the borg repository. The contents of the keyfile is encrypted with a passphrase. The keyfile needs to be re-created on the system performing the recovery operation so that it can decrypt the backup. Open /root/.config/borg/keys/mnt_backup_borg in an editor, create if not present. The directory may need to be created using the following command. mkdir -p /root/.config/borg/keys Once entered, save the contents of the file. Remount Borg Repository The Borg repository needs to be mounted at same location for borg mapping to work correctly. mkdir -p /mnt/backup mount -t ext4 /dev/sdb1 /mnt/backup View Contents The borg list command provides a listing of the archives present in the backup. root@thinkpad:borg list /mnt/backup/borg/ Enter passphrase for key /root/.config/borg/keys/mnt_backup_borg: vcenter-2022-12-01T21:07:04 Thu, 2022-12-01 21:07:04 [7b6e2497152015654c5b6aed44dbb81ed276b0218666969e74fc03fb2e81f0f4] vcenter-2022-12-04T22:03:00 Sun, 2022-12-04 22:03:01 [dd441e60a28c58aa5e30fc2dec2345f00389b47626a378c33d25c53be52dbb74] vcenter-2022-12-05T22:00:12 Mon, 2022-12-05 22:00:13 [8612f5b8a1f72176d492d1f7c78cd8db1a0b90bbb7eb41a796208e4f6a8dd432] vcenter-2022-12-06T22:03:20 Tue, 2022-12-06 22:03:21 [a991b553e0ac8584e71297b4f284702a30c89d583f814362fe9fcbd89d97e5d8] vcenter-2022-12-07T22:00:38 Wed, 2022-12-07 22:00:38 [49eddca0e4cd2b4c6c1f8096beb6a3f84e0220c6fdfc6a7a226eb1f3205d997d] vcenter-2022-12-09T22:02:57 Fri, 2022-12-09 22:02:58 [5e5304f3f50a864729376b11d97b64d1d05b7846aa802c5de7b05ddfa8d7357a] vcenter-2022-12-10T22:03:02 Sat, 2022-12-10 22:03:05 [8a37b7ed495dc42cc9cd7a6f867c3d03628a021c7b2807b34f877bbb0665bb7a] vcenter-2022-12-11T22:02:17 Sun, 2022-12-11 22:02:18 [1ac856f2ba227912ac54b50b83a70129538c04a63e3c3ef9b63fbfb6a715d943] vcenter-2022-12-12T22:00:16 Mon, 2022-12-12 22:00:17 [53a3cd34b5eaebe3716cd048b696125a077eba21ce239bf56be902d90f0b795b] Each archive can be mounted into file-system and navigated. mkdir -p /mnt/extract/borg borg mount /mnt/backup/borg::<name from first column>","title":"Borg"},{"location":"borg/#borg-backup","text":"Borg provides facilitites for backing up data through efficient means through features such as deduplication and compression. Borg places all backups for a given job within a repository. These repositories then contain multiple archives (e.g. single backup). More information can be found on the docs here .","title":"Borg Backup"},{"location":"borg/#installation","text":"Installation is easy and available by default within ubuntu repositories. sudo apt-get install borgbackup","title":"Installation"},{"location":"borg/#setting-up-backup","text":"There are several steps to getting a borg repository configured and enabling backups. Creating Repository Backing Keying Material Create Archives Each of the sections are discussed in greater detail.","title":"Setting Up Backup"},{"location":"borg/#creating-repository","text":"A Borg repository contains the backup archives that are created. In general, the repos will be encrypted with keys that are protected by a passphrase. There are 2 options for creating the repository. First, the wrapped key blob can be stored within the repo itself alongside the archives. This simplifies the restoration process as the keys are within the repository. This also means that the passphrase is only protection for data. Repos with contained keys can be created with the following borg init --encryption repokey /mnt/backup/borg The second option is called keyfile as opposed to repokey. The keyfile solution will install the wrapped keys into the server as opposed to the repo itself. This results in more secure posture as the wrapped keys and the corresponding passphrase would be required for unlocking the repos. borg init --encryption keyfile /mnt/backup/borg","title":"Creating Repository"},{"location":"borg/#keying-material-backup","text":"The keyfile option from previous step results in the keys being stored within the server. As such, it also becomes necessary to backup the keys from the server so that it can be restored should the server crash. borg key export /mnt/backup/borg Once exported, the key should be added to Bitwarden for archiving","title":"Keying Material Backup"},{"location":"borg/#create-archives-backups-as-desired","text":"Borg calls a backup an archive within their documentation. Creating an archive in Borg parlance is equivalent to running a backup. The following command can be used to create a backup of /mnt/critical into a repo located at /mnt/backup/borg . borg create \\ --verbose \\ --filter AME \\ --list \\ --progress \\ --stats \\ --show-rc \\ --compression zstd,10 \\ --exclude-caches \\ --exclude /mnt/critical/ignore \\ \\ /mnt/backup/borg::'critical-{now}' \\ /mnt/critical","title":"Create archives (backups) as desired"},{"location":"borg/#recovery-process","text":"Steps 1. Install borgbackup on client 2. Download borg repository from offsite location 3. Re-create keyfile within recovery system 4. Mount Recovery drive at same path (/mnt/backup/borg)","title":"Recovery Process"},{"location":"borg/#re-create-keyfile-from-bitwarden","text":"The keyfile contains the decryption/MAC keys for the borg repository. The contents of the keyfile is encrypted with a passphrase. The keyfile needs to be re-created on the system performing the recovery operation so that it can decrypt the backup. Open /root/.config/borg/keys/mnt_backup_borg in an editor, create if not present. The directory may need to be created using the following command. mkdir -p /root/.config/borg/keys Once entered, save the contents of the file.","title":"Re-create keyfile from Bitwarden"},{"location":"borg/#remount-borg-repository","text":"The Borg repository needs to be mounted at same location for borg mapping to work correctly. mkdir -p /mnt/backup mount -t ext4 /dev/sdb1 /mnt/backup View Contents The borg list command provides a listing of the archives present in the backup. root@thinkpad:borg list /mnt/backup/borg/ Enter passphrase for key /root/.config/borg/keys/mnt_backup_borg: vcenter-2022-12-01T21:07:04 Thu, 2022-12-01 21:07:04 [7b6e2497152015654c5b6aed44dbb81ed276b0218666969e74fc03fb2e81f0f4] vcenter-2022-12-04T22:03:00 Sun, 2022-12-04 22:03:01 [dd441e60a28c58aa5e30fc2dec2345f00389b47626a378c33d25c53be52dbb74] vcenter-2022-12-05T22:00:12 Mon, 2022-12-05 22:00:13 [8612f5b8a1f72176d492d1f7c78cd8db1a0b90bbb7eb41a796208e4f6a8dd432] vcenter-2022-12-06T22:03:20 Tue, 2022-12-06 22:03:21 [a991b553e0ac8584e71297b4f284702a30c89d583f814362fe9fcbd89d97e5d8] vcenter-2022-12-07T22:00:38 Wed, 2022-12-07 22:00:38 [49eddca0e4cd2b4c6c1f8096beb6a3f84e0220c6fdfc6a7a226eb1f3205d997d] vcenter-2022-12-09T22:02:57 Fri, 2022-12-09 22:02:58 [5e5304f3f50a864729376b11d97b64d1d05b7846aa802c5de7b05ddfa8d7357a] vcenter-2022-12-10T22:03:02 Sat, 2022-12-10 22:03:05 [8a37b7ed495dc42cc9cd7a6f867c3d03628a021c7b2807b34f877bbb0665bb7a] vcenter-2022-12-11T22:02:17 Sun, 2022-12-11 22:02:18 [1ac856f2ba227912ac54b50b83a70129538c04a63e3c3ef9b63fbfb6a715d943] vcenter-2022-12-12T22:00:16 Mon, 2022-12-12 22:00:17 [53a3cd34b5eaebe3716cd048b696125a077eba21ce239bf56be902d90f0b795b] Each archive can be mounted into file-system and navigated. mkdir -p /mnt/extract/borg borg mount /mnt/backup/borg::<name from first column>","title":"Remount Borg Repository"},{"location":"kvm/","text":"Virtual Machines Creation Creation of virtual machines handled through virt-install command. The virt-install command is installed as part of the virt-manager package. The following is an example of creating a virtual machine. virt-install \\ --connect qemu:///system \\ --virt-type kvm \\ --name ns1 \\ --vcpus 1 \\ --ram 1024 \\ --disk path=/mnt/internal/vm_store/disks/ns1-disk.img,size=20 \\ --network bridge=br0 \\ --graphics vnc,listen=0.0.0.0 \\ --cdrom /mnt/internal/vm_store/iso/ubuntu-22.04.1-live-server-amd64.iso \\ --os-variant ubuntu21.04 The previous command will start the virtual machine and create a VNC server on port 5900 (default) for interacting with machines console. Downloading VNC client and pointing to the host on port 5900 will provide access to an interactive console. The creation process is ran by libvirt-qemu user within the system. As such, the iso files and locations for storing the disks must be accessible by libvirt-qemu. Permissions errors will result in failure to create the virtual machine. root@vcenter:/etc/netplan# ls -lah /mnt/internal/vm_store/ total 3.4M drwxrwx--- 4 libvirt-qemu kvm 4.0K Nov 4 02:57 . drwxr-xr-x 4 cameron kvm 4.0K Nov 4 02:30 .. drwxrwx--- 2 libvirt-qemu kvm 4.0K Nov 5 21:58 disks drwxrwx--- 2 libvirt-qemu kvm 4.0K Nov 4 02:11 iso -rwxrwx--- 1 libvirt-qemu kvm 21G Nov 4 02:50 ubuntu22.04 Deletion Start/Stop/Reboot Machine Machines that are running can be listed using the virsh list command. root@vcenter:/etc/netplan# virsh list --all Id Name State ---------------------- 11 ns1 running ``` Virtual machines can be stopped using the virsh poweroff command when desired. ```bash virsh poweroff ns1 Machines can be started with virsh start virsh start ns1 Shared Directories Sharing directories between the host and guests will be necessary to support RAID storage. The following needs to be added to the virt-install command when the machine is created to support shared directories. --filesystem /mnt/critical/source,/sharename With the fileystem argument added the command would look like the following: virt-install \\ --connect qemu:///system \\ --virt-type kvm \\ --name ns1 \\ --vcpus 1 \\ --ram 1024 \\ --disk path=/mnt/internal/vm_store/disks/ns1-disk.img,size=20 \\ --network bridge=br0 \\ --graphics vnc,listen=0.0.0.0 \\ --filesystem /mnt/local/path,/sharename \\ --cdrom /mnt/internal/vm_store/iso/ubuntu-22.04.1-live-server-amd64.iso \\ --os-variant ubuntu21.04 This will create a share within the guest called \"sharename\" that can then be mounted within the guest VM. There are two methods by which the directory can be mounted inside the VM. First, the share can be mounted manually with the following commands. mkdir -p /path/in/host sudo mount -t 9p -o trans=virtio /sharename /path/in/host The second option is to update fstab so that it automatically mounts each time the VM starts. The following line could be added to the /etc/fstab to support automated mounting. /sharename /path/in/host 9p trans=virtio,version=9p2000.L,rw 0 0 This will create a mount within the guest that allows for saving files that will become available within the host.","title":"KVM"},{"location":"kvm/#virtual-machines","text":"","title":"Virtual Machines"},{"location":"kvm/#creation","text":"Creation of virtual machines handled through virt-install command. The virt-install command is installed as part of the virt-manager package. The following is an example of creating a virtual machine. virt-install \\ --connect qemu:///system \\ --virt-type kvm \\ --name ns1 \\ --vcpus 1 \\ --ram 1024 \\ --disk path=/mnt/internal/vm_store/disks/ns1-disk.img,size=20 \\ --network bridge=br0 \\ --graphics vnc,listen=0.0.0.0 \\ --cdrom /mnt/internal/vm_store/iso/ubuntu-22.04.1-live-server-amd64.iso \\ --os-variant ubuntu21.04 The previous command will start the virtual machine and create a VNC server on port 5900 (default) for interacting with machines console. Downloading VNC client and pointing to the host on port 5900 will provide access to an interactive console. The creation process is ran by libvirt-qemu user within the system. As such, the iso files and locations for storing the disks must be accessible by libvirt-qemu. Permissions errors will result in failure to create the virtual machine. root@vcenter:/etc/netplan# ls -lah /mnt/internal/vm_store/ total 3.4M drwxrwx--- 4 libvirt-qemu kvm 4.0K Nov 4 02:57 . drwxr-xr-x 4 cameron kvm 4.0K Nov 4 02:30 .. drwxrwx--- 2 libvirt-qemu kvm 4.0K Nov 5 21:58 disks drwxrwx--- 2 libvirt-qemu kvm 4.0K Nov 4 02:11 iso -rwxrwx--- 1 libvirt-qemu kvm 21G Nov 4 02:50 ubuntu22.04","title":"Creation"},{"location":"kvm/#deletion","text":"","title":"Deletion"},{"location":"kvm/#startstopreboot-machine","text":"Machines that are running can be listed using the virsh list command. root@vcenter:/etc/netplan# virsh list --all Id Name State ---------------------- 11 ns1 running ``` Virtual machines can be stopped using the virsh poweroff command when desired. ```bash virsh poweroff ns1 Machines can be started with virsh start virsh start ns1","title":"Start/Stop/Reboot Machine"},{"location":"kvm/#shared-directories","text":"Sharing directories between the host and guests will be necessary to support RAID storage. The following needs to be added to the virt-install command when the machine is created to support shared directories. --filesystem /mnt/critical/source,/sharename With the fileystem argument added the command would look like the following: virt-install \\ --connect qemu:///system \\ --virt-type kvm \\ --name ns1 \\ --vcpus 1 \\ --ram 1024 \\ --disk path=/mnt/internal/vm_store/disks/ns1-disk.img,size=20 \\ --network bridge=br0 \\ --graphics vnc,listen=0.0.0.0 \\ --filesystem /mnt/local/path,/sharename \\ --cdrom /mnt/internal/vm_store/iso/ubuntu-22.04.1-live-server-amd64.iso \\ --os-variant ubuntu21.04 This will create a share within the guest called \"sharename\" that can then be mounted within the guest VM. There are two methods by which the directory can be mounted inside the VM. First, the share can be mounted manually with the following commands. mkdir -p /path/in/host sudo mount -t 9p -o trans=virtio /sharename /path/in/host The second option is to update fstab so that it automatically mounts each time the VM starts. The following line could be added to the /etc/fstab to support automated mounting. /sharename /path/in/host 9p trans=virtio,version=9p2000.L,rw 0 0 This will create a mount within the guest that allows for saving files that will become available within the host.","title":"Shared Directories"},{"location":"pihole/","text":"PiHole Enabling Transport Layer Security (TLS) By default, the PiHole docker image utilizes http for communications through the browser. The http protocol provides no protections from confidentiality perspective. As such, the login credentials for the administrative interface are communicated in the clear. This allows an attacker with access to the network communications to capture the passwords for the system. The docker image can be configured to support Tranpsort Layer Security (TLS) so that all communications with the service are encrypted and protected from tampering. The following is a high level overview of the steps necessary to enable TLS Decide on a Domain for the DNS system (ns.domain.home for example) Issue a TLS certificate off the appropriate certificate authority (see network-keying repository) Reformat the TLS certificate and corresponding key so that PiHole able to understand Update lighttpd configuration to enable TLS and find TLS certificate in container Update docker and docker-compose to make TLS certificate available Update docker-compose to allow port 443/tcp access externally (defaults to 80/tcp) Navigate to https://ns.domain.home/admin to utilize web password to login Issue TLS Certificate and Install The TLS certificate must come from a Certificate Authority (CA) that is trusted across all the clients/devices in the network. The Root Certificate from the CA must be installed within the CA trust stores across all the devices and browsers to enable support. There are 2 primary mechanisms by which a TLS Certificate can be generated with differing levels of protection. The first option is to generate a private/public key on the server that will be hosting the PiHole infrastructure. Once generated, the private/public key should be utilized to generate a Certificate Signing Request (CSR). The CSR would then be utilized with the CA's private key to generate a certificate. The benefit of this option is that the private key associated with the certificate is known only to the machine that will be utilizing the certificate. The second option is to leverage the infrastructure within the network-keying repository to generate a new certificate and private key. The private key and certificate would then get uploaded, through secure channel, to the server that will be hosting the PiHole infrastructure. Once transferred to the server, the copy of the private key on the machine performing the generation should be securely deleted. Once the private key and certificate are available, the files will need to be combined into a single file so that lighttpd understand how to interpret the key and certificate. The following reference command can be utilized to combined the files. cat ns.domain.home.key ns.domain.home.cert | tee ns.domain.home-combined.pem The previous command utilizes the cat command to append the certificate to the end of the private key before placing the results into a new file. The TLS certificate/key needs to be installed into location where it can be bind-mounted into the docker container. At the time of this writing, the certificates are being installed to /opt/stark/certs and permissions adjusted for minimizing access. Lighttpd Configuration Updates The lighttpd web server needs to be configured to enable TLS communications and be directed to the utilize the TLS certificate. The lighttpd gets updated with various settings to enable the use of TLS but the one of interest for pointing at the certificate is as follows: ssl.pemfile = \"/ssl/ns.domain.home-combined.pem\" That line indicates the location within the docker container where the TLS certificate created in previous step must be available. The lighttpd browser will utilize this path when attempting to perform TLS communications. The lighttpd-updates.conf file within the network-services repository contains configurations that were made to enable support and restrict ciphers appropriately. Docker and Docker Compose Updates By default, the PiHole application makes use of http for communications which means that port 80 is exposed. HTTPS is utilized when the server supports TLS and the client connects securely. The HTTPS protocol requires that port 443 becomes available on the container as well. The docker-compose.yaml must be updated to include port 443 such as the following. ports: - \"53:53/tcp\" - \"53:53/udp\" - \"80:80/tcp\" - \"443:443/tcp\" Next, the docker-compose needs updated to include bind-mount for the TLS certificate so that it can be seen inside the container. The following shows an example of that part. volumes: - '/mnt/critical/dns/cfg:/etc/pihole' - '/mnt/critical/dns/dnsmasq:/etc/dnsmasq.d' - '/opt/stark/certs/ns.domain.home-combined.pem:/ssl/ns.domain.home-combined.pem:ro' The last line in the previous snippet shows the bind-mount that provides that level of access. Finally, the docker build process was updated to modify the lighttpd configuration file to enable TLS support. This last step is necessary so that lighttpd actually knows to look for the certificate and enable TLS.","title":"PiHole"},{"location":"pihole/#pihole","text":"","title":"PiHole"},{"location":"pihole/#enabling-transport-layer-security-tls","text":"By default, the PiHole docker image utilizes http for communications through the browser. The http protocol provides no protections from confidentiality perspective. As such, the login credentials for the administrative interface are communicated in the clear. This allows an attacker with access to the network communications to capture the passwords for the system. The docker image can be configured to support Tranpsort Layer Security (TLS) so that all communications with the service are encrypted and protected from tampering. The following is a high level overview of the steps necessary to enable TLS Decide on a Domain for the DNS system (ns.domain.home for example) Issue a TLS certificate off the appropriate certificate authority (see network-keying repository) Reformat the TLS certificate and corresponding key so that PiHole able to understand Update lighttpd configuration to enable TLS and find TLS certificate in container Update docker and docker-compose to make TLS certificate available Update docker-compose to allow port 443/tcp access externally (defaults to 80/tcp) Navigate to https://ns.domain.home/admin to utilize web password to login","title":"Enabling Transport Layer Security (TLS)"},{"location":"pihole/#issue-tls-certificate-and-install","text":"The TLS certificate must come from a Certificate Authority (CA) that is trusted across all the clients/devices in the network. The Root Certificate from the CA must be installed within the CA trust stores across all the devices and browsers to enable support. There are 2 primary mechanisms by which a TLS Certificate can be generated with differing levels of protection. The first option is to generate a private/public key on the server that will be hosting the PiHole infrastructure. Once generated, the private/public key should be utilized to generate a Certificate Signing Request (CSR). The CSR would then be utilized with the CA's private key to generate a certificate. The benefit of this option is that the private key associated with the certificate is known only to the machine that will be utilizing the certificate. The second option is to leverage the infrastructure within the network-keying repository to generate a new certificate and private key. The private key and certificate would then get uploaded, through secure channel, to the server that will be hosting the PiHole infrastructure. Once transferred to the server, the copy of the private key on the machine performing the generation should be securely deleted. Once the private key and certificate are available, the files will need to be combined into a single file so that lighttpd understand how to interpret the key and certificate. The following reference command can be utilized to combined the files. cat ns.domain.home.key ns.domain.home.cert | tee ns.domain.home-combined.pem The previous command utilizes the cat command to append the certificate to the end of the private key before placing the results into a new file. The TLS certificate/key needs to be installed into location where it can be bind-mounted into the docker container. At the time of this writing, the certificates are being installed to /opt/stark/certs and permissions adjusted for minimizing access.","title":"Issue TLS Certificate and Install"},{"location":"pihole/#lighttpd-configuration-updates","text":"The lighttpd web server needs to be configured to enable TLS communications and be directed to the utilize the TLS certificate. The lighttpd gets updated with various settings to enable the use of TLS but the one of interest for pointing at the certificate is as follows: ssl.pemfile = \"/ssl/ns.domain.home-combined.pem\" That line indicates the location within the docker container where the TLS certificate created in previous step must be available. The lighttpd browser will utilize this path when attempting to perform TLS communications. The lighttpd-updates.conf file within the network-services repository contains configurations that were made to enable support and restrict ciphers appropriately.","title":"Lighttpd Configuration Updates"},{"location":"pihole/#docker-and-docker-compose-updates","text":"By default, the PiHole application makes use of http for communications which means that port 80 is exposed. HTTPS is utilized when the server supports TLS and the client connects securely. The HTTPS protocol requires that port 443 becomes available on the container as well. The docker-compose.yaml must be updated to include port 443 such as the following. ports: - \"53:53/tcp\" - \"53:53/udp\" - \"80:80/tcp\" - \"443:443/tcp\" Next, the docker-compose needs updated to include bind-mount for the TLS certificate so that it can be seen inside the container. The following shows an example of that part. volumes: - '/mnt/critical/dns/cfg:/etc/pihole' - '/mnt/critical/dns/dnsmasq:/etc/dnsmasq.d' - '/opt/stark/certs/ns.domain.home-combined.pem:/ssl/ns.domain.home-combined.pem:ro' The last line in the previous snippet shows the bind-mount that provides that level of access. Finally, the docker build process was updated to modify the lighttpd configuration file to enable TLS support. This last step is necessary so that lighttpd actually knows to look for the certificate and enable TLS.","title":"Docker and Docker Compose Updates"},{"location":"raid/","text":"RAID Data stored within the network is stored within a series of RAID arrays. RAID, or Redundant Array of Inexpensive Disks, is a technology where multiple physical disks are utilized to prevent data loss in event of disk failure. There are several modes or types of RAID that can be utilized with varying levels of advantages and disadvantages. The RAIDs in place as of now are configured in a RAID-1 (or mirrored) configuration. This means that any data written to the RAID are written across all the physical disks in the RAID. RAIDs can be implemented within Hardware or Software. There are various trade-offs that come into play when considerig whether to utilize hardware or software. Criteria Software Hardware Performance Slower performance as CPU utilized. Higher performance as dedicated hardware used. Complexity of Administration Increased complexity as admin responsible for all aspects such as creation, monitoring, and disk management. Simpler as admin only responsible for enable data writes to drives. Vendor Lock-in No Vendor Lock Common for hardware controllers to utilize different formats that are incompatible. Alerting Capabilities Scripting and automation can be developed to detect failed disks and notify admins. Hardware controllers responsible for detection and alerting restricted to what is enabled by hardware. Flexibility Most flexible as RAIDs are configured and managed within software. As such, custom setups are different combinations of commands. No flexibility beyond what is configured within hardware controller firmware. Support bug fixes and updates Very supportive as it's just a matter of updating a software package. Not nearly as flexible as it's a hardware firmware updates. Dependent upon hardware developers maintaining their product lines for full length of time that it's present in your environment. Security Support software updates to fix bugs. Security updates may never happen depending upon controllers ability to reload. The trade-offs documented in the above table was considered when choosing whether to utilize software or hardware based RAID. The flexibility, lack of vendor lock-in, and increased alerting capabilities of the software RAID resulted in it being chosen for this network. There are 2 raids configured within the network to manage the storage of data. The first raid is accessible at /mnt/critical within the server environment. This serves as the primary storage location for the services. The second raid is /mnt/backup which is larger than /mnt/critical and services to store multiple years worth of backups. As mentioned above, these RAIDs are both running in a RAID-1 configuration with 2 physical disks present in each. This provides redundancy for disk failure in either of these two raids. Setup These setup notes will walk through the process of setting up a software RAID using the mdadm package from linux. The first step in setting up a RAID is to install the necessary packages. This can be completed with the following commands. sudo apt-get update sudo apt-get install mdadm The next step is to identify which devices within the system need to be included in the RAID. The lsblk command can be utilized to list all the block devices (hard disks) within the server environment. lsblk This command will provide a list of all the devices and their respective sizes. This information should help to identify the block device names that will need to be utilized in the next step. After identifying the devices, the next step is to create the RAID using those physical devices. The following is an example of that command for devices /dev/sdb1 and /dev/sdc1. mdadm --create --verbose /dev/md/<name> --level=1 --raid-devices=2 /dev/<dev 1> /dev/<dev 2> Ex. mdadm --create --verbose /dev/md/critical --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1 The above example created what is called a multi-device (md) in the MDADM terminology. The MD device acts as a virtual device that can be written to or read from within Linux environment. All actions on the MD device will span across the physical devices added to the RAID. The name given to the MD in the above example is /dev/md/critical as it corresponded to the critical RAID. Checking Status The status of the RAIDs can be checked through a couple different avenues. The first path is to cat the /proc/mdstat file within the Linux environment. The following is an example of the output that becomes available as part of reading that virtual file. cameron@vcenter:~$ cat /proc/mdstat Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md126 : active raid1 sdd1[1] sdc1[0] 3906876416 blocks super 1.2 [2/2] [UU] bitmap: 0/30 pages [0KB], 65536KB chunk md127 : active raid1 sdf1[1] sde1[0] 976620544 blocks super 1.2 [2/2] [UU] bitmap: 0/8 pages [0KB], 65536KB chunk There's a detail command that can be leveraged with mdadm to get even more information on a particular RAID. The following is an example of looking a bit more in depth at a particular RAID. root@vcenter:/home/cameron# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Mon Apr 12 11:20:41 2021 Raid Level : raid1 Array Size : 976620544 (931.38 GiB 1000.06 GB) Used Dev Size : 976620544 (931.38 GiB 1000.06 GB) Raid Devices : 2 Total Devices : 2 Persistence : Superblock is persistent Intent Bitmap : Internal Update Time : Tue Mar 21 21:29:09 2023 State : clean Active Devices : 2 Working Devices : 2 Failed Devices : 0 Spare Devices : 0 Consistency Policy : bitmap Name : www:critical UUID : 2b068395:2f23a3ef:e6bbe535:51acfb80 Events : 59106 Number Major Minor RaidDevice State 0 8 65 0 active sync /dev/sde1 1 8 81 1 active sync /dev/sdf1 As can be seen, there are a lot more details available from the use of that command. This page will also provide details if the RAID happens to be going through a re-synchronization process. It's important to note that the device created with the above commands does not yet have a file-system or structure to data on it. It's effectively a RAW block device. As such, it will be necessary to create a file-system on the md for it to be utilized for storage. The following in an example of creating a file-system on the MD and subsequently mounting it to /mnt/critical. However, the following will not add any sort of encryption on the data stored on the disks. For encryption, the steps in the LUKS section should be followed. mkfs.ext4 /dev/md/critical mkdir -p /mnt/critical mount -t ext4 /dev/md/critical /mnt/critical There are also changes that could be made to /etc/fstab that would handle the auto-mount of this on the next reboot. Maintenance and Performance There are some configurable options with RAID to keep in mind. Check Operations Frequency The check operations are typically quite resource intensive to RAIDs. The process involves walking the contents of the RAID and ensuring that all drives maintain the appropriate data. This can significantly impact the availability of resources and response times for the server. The following command will list the scheduling for when an automated check is performed. Scheduling for a time with lower usage is ideal from an impact perspective. sudo systemctl list-timers mdcheck_start This will result in a message like the following: NEXT LEFT LAST PASSED UNIT ACTIVATES Sun 2023-05-07 13:51:06 CDT 4 weeks 0 days left Sun 2023-04-02 07:54:24 CDT 1 week 0 days ago mdcheck_start.timer mdcheck_start.service 1 timers listed. Pass --all to see loaded but inactive timers, too. The above lists the frequency in which automated check operations are being performed. Speeds for Check and Rebuild Operations There are speed limitations that the software utilizes as a goal when handling RAID operations. There is a minimum and maximum speed associated with the RAID. sudo sysctl dev.raid.speed_limit_min sudo sysctl dev.raid.speed_limit_max The following is an example of the default values that are configured within the software. root@vcenter: sysctl dev.raid.speed_limit_min dev.raid.speed_limit_min = 1000 root@vcenter: sysctl dev.raid.speed_limit_max dev.raid.speed_limit_max = 200000 The values can be changed using the systemctl command as well. The numbers are in kilobytes/second on a per-device basis. The following will set a limit of around 7 MB/sec. sudo sysctl -w dev.raid.speed_limit_max=7000 The following are some values based upon the number of drives. The /etc/sysctl.conf file contains the default values for the system. The following is an exmaple of the content. #################NOTE ################ ## You are limited by CPU and memory too # ########################################### dev.raid.speed_limit_min = 50000 ## good for 4-5 disks based array ## dev.raid.speed_limit_max = 2000000 ## good for large 6-12 disks based array ### dev.raid.speed_limit_max = 5000000 Linux Unified Key Setup (LUKS) Another important decision is to whether the data being written to the RAID will be protected through the use of encryption. The use of encryption with the RAID ensures that all data placed on the RAID will not be readable by unauthorized individuals that may have physical access to the drives. Disk encryption within Linux commonly utilizes a combination of dm-crypt and Linux Unified Key Setup (LUKS). The dm-crypt module handles the cryptographic operations while LUKS provides the key management needed for dm-crypt to operate. Setup Integration with RAID devices configured in previous section would enable the ability to have redundancy and protection of data. Once enabled, a password will need to be provided each time the RAID is mounted. Enabling protection also means that it won't be possible for the system to automatically recover in event of power loss. The following command will initiate the creation process on the /dev/md/critical block device. cryptsetup -y -v luksFormat /dev/md/critical A prompt for password entry will be appear. Be sure to keep the password in safe place as there is no recovery path if forgotten. Once completed, the RAID device will be encrypted but there won't be a file-system present. Also, the encrypted volume will need to be mapped into the file hierarchy before it can be utilized. The following command will mount the encrypted partition to the provided mapper point. In this case, the mapper's name is called secret. cryptsetup luksOpen /dev/md/critical <mapper_point> Ex. cryptsetup luksOpen /dev/md/critical secret By default, the partition creation process does not fill all the drive's contents. This means that any data previously stored on the drive is still present. This can be alleviated by fill the secret partition with random data that covers entire drive. pv -tpreb /dev/zero | dd of=/dev/mapper/secret bs=128M The filling process will take a long time and will vary depending on the speed and size of the drives. Finally, the last step in the setup process is to create a file-system on the RAID device. This will allow for mounting the encryption volume/RAID into the file-system and provide transparent encryption and redundancy in store. The following command will create an ext4 file-system within the encrypted partition. mkfs.ext4 /dev/mapper/secret Once the file-system is created it will need to be mounted into the file hierarchy. mount /dev/mapper/secret /mnt/critical Unlocking Process There are 2 steps associated with unlocking an encrypted partition using LUKS. Encrypted Partition needs to be mounted to an accessible block device. File-system on the device must be mounted to become accessible. The commands seen above show an example of the process but a more targeted combination are provided for preference. # Mount Block Device cryptsetup luksOpen /dev/md/critical secret # Mount file-system mount -t ext4 /dev/mapper/secret /mnt/critical The previous shows the process for making the encrypted partition available for file storage at the /mnt/critical location. Locking Process The following is the process for locking the RAID devices to prevent data corruption on power down. The following sequence should be followed when shutting down the system to ensure that everything remains in consistent state. # Unmount the file-system to prevent use umount /mnt/critical # Close encrypted partitiont to remove key from memory cryptsetup luksClose secret Key Backup The LUKS package utilizes a series of headers to store the decryption keys in a wrapped format. The password that is entered is utilized to derive a key that is utilized to protect the actual encryption key for the partition. This design allows for changing of password without requiring the entire drive to be re-encrypted. The LUKS header can actually maintain several different passwords that will unlock the disk. This is done through a series of slots that can be utilized for storing the wrapped keys. The following command can be utilized to dump the header associated with the LUKS partition. cryptsetup luksDump /dev/mapper/secret","title":"RAID"},{"location":"raid/#raid","text":"Data stored within the network is stored within a series of RAID arrays. RAID, or Redundant Array of Inexpensive Disks, is a technology where multiple physical disks are utilized to prevent data loss in event of disk failure. There are several modes or types of RAID that can be utilized with varying levels of advantages and disadvantages. The RAIDs in place as of now are configured in a RAID-1 (or mirrored) configuration. This means that any data written to the RAID are written across all the physical disks in the RAID. RAIDs can be implemented within Hardware or Software. There are various trade-offs that come into play when considerig whether to utilize hardware or software. Criteria Software Hardware Performance Slower performance as CPU utilized. Higher performance as dedicated hardware used. Complexity of Administration Increased complexity as admin responsible for all aspects such as creation, monitoring, and disk management. Simpler as admin only responsible for enable data writes to drives. Vendor Lock-in No Vendor Lock Common for hardware controllers to utilize different formats that are incompatible. Alerting Capabilities Scripting and automation can be developed to detect failed disks and notify admins. Hardware controllers responsible for detection and alerting restricted to what is enabled by hardware. Flexibility Most flexible as RAIDs are configured and managed within software. As such, custom setups are different combinations of commands. No flexibility beyond what is configured within hardware controller firmware. Support bug fixes and updates Very supportive as it's just a matter of updating a software package. Not nearly as flexible as it's a hardware firmware updates. Dependent upon hardware developers maintaining their product lines for full length of time that it's present in your environment. Security Support software updates to fix bugs. Security updates may never happen depending upon controllers ability to reload. The trade-offs documented in the above table was considered when choosing whether to utilize software or hardware based RAID. The flexibility, lack of vendor lock-in, and increased alerting capabilities of the software RAID resulted in it being chosen for this network. There are 2 raids configured within the network to manage the storage of data. The first raid is accessible at /mnt/critical within the server environment. This serves as the primary storage location for the services. The second raid is /mnt/backup which is larger than /mnt/critical and services to store multiple years worth of backups. As mentioned above, these RAIDs are both running in a RAID-1 configuration with 2 physical disks present in each. This provides redundancy for disk failure in either of these two raids.","title":"RAID"},{"location":"raid/#setup","text":"These setup notes will walk through the process of setting up a software RAID using the mdadm package from linux. The first step in setting up a RAID is to install the necessary packages. This can be completed with the following commands. sudo apt-get update sudo apt-get install mdadm The next step is to identify which devices within the system need to be included in the RAID. The lsblk command can be utilized to list all the block devices (hard disks) within the server environment. lsblk This command will provide a list of all the devices and their respective sizes. This information should help to identify the block device names that will need to be utilized in the next step. After identifying the devices, the next step is to create the RAID using those physical devices. The following is an example of that command for devices /dev/sdb1 and /dev/sdc1. mdadm --create --verbose /dev/md/<name> --level=1 --raid-devices=2 /dev/<dev 1> /dev/<dev 2> Ex. mdadm --create --verbose /dev/md/critical --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1 The above example created what is called a multi-device (md) in the MDADM terminology. The MD device acts as a virtual device that can be written to or read from within Linux environment. All actions on the MD device will span across the physical devices added to the RAID. The name given to the MD in the above example is /dev/md/critical as it corresponded to the critical RAID.","title":"Setup"},{"location":"raid/#checking-status","text":"The status of the RAIDs can be checked through a couple different avenues. The first path is to cat the /proc/mdstat file within the Linux environment. The following is an example of the output that becomes available as part of reading that virtual file. cameron@vcenter:~$ cat /proc/mdstat Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md126 : active raid1 sdd1[1] sdc1[0] 3906876416 blocks super 1.2 [2/2] [UU] bitmap: 0/30 pages [0KB], 65536KB chunk md127 : active raid1 sdf1[1] sde1[0] 976620544 blocks super 1.2 [2/2] [UU] bitmap: 0/8 pages [0KB], 65536KB chunk There's a detail command that can be leveraged with mdadm to get even more information on a particular RAID. The following is an example of looking a bit more in depth at a particular RAID. root@vcenter:/home/cameron# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Mon Apr 12 11:20:41 2021 Raid Level : raid1 Array Size : 976620544 (931.38 GiB 1000.06 GB) Used Dev Size : 976620544 (931.38 GiB 1000.06 GB) Raid Devices : 2 Total Devices : 2 Persistence : Superblock is persistent Intent Bitmap : Internal Update Time : Tue Mar 21 21:29:09 2023 State : clean Active Devices : 2 Working Devices : 2 Failed Devices : 0 Spare Devices : 0 Consistency Policy : bitmap Name : www:critical UUID : 2b068395:2f23a3ef:e6bbe535:51acfb80 Events : 59106 Number Major Minor RaidDevice State 0 8 65 0 active sync /dev/sde1 1 8 81 1 active sync /dev/sdf1 As can be seen, there are a lot more details available from the use of that command. This page will also provide details if the RAID happens to be going through a re-synchronization process. It's important to note that the device created with the above commands does not yet have a file-system or structure to data on it. It's effectively a RAW block device. As such, it will be necessary to create a file-system on the md for it to be utilized for storage. The following in an example of creating a file-system on the MD and subsequently mounting it to /mnt/critical. However, the following will not add any sort of encryption on the data stored on the disks. For encryption, the steps in the LUKS section should be followed. mkfs.ext4 /dev/md/critical mkdir -p /mnt/critical mount -t ext4 /dev/md/critical /mnt/critical There are also changes that could be made to /etc/fstab that would handle the auto-mount of this on the next reboot.","title":"Checking Status"},{"location":"raid/#maintenance-and-performance","text":"There are some configurable options with RAID to keep in mind.","title":"Maintenance and Performance"},{"location":"raid/#check-operations-frequency","text":"The check operations are typically quite resource intensive to RAIDs. The process involves walking the contents of the RAID and ensuring that all drives maintain the appropriate data. This can significantly impact the availability of resources and response times for the server. The following command will list the scheduling for when an automated check is performed. Scheduling for a time with lower usage is ideal from an impact perspective. sudo systemctl list-timers mdcheck_start This will result in a message like the following: NEXT LEFT LAST PASSED UNIT ACTIVATES Sun 2023-05-07 13:51:06 CDT 4 weeks 0 days left Sun 2023-04-02 07:54:24 CDT 1 week 0 days ago mdcheck_start.timer mdcheck_start.service 1 timers listed. Pass --all to see loaded but inactive timers, too. The above lists the frequency in which automated check operations are being performed.","title":"Check Operations Frequency"},{"location":"raid/#speeds-for-check-and-rebuild-operations","text":"There are speed limitations that the software utilizes as a goal when handling RAID operations. There is a minimum and maximum speed associated with the RAID. sudo sysctl dev.raid.speed_limit_min sudo sysctl dev.raid.speed_limit_max The following is an example of the default values that are configured within the software. root@vcenter: sysctl dev.raid.speed_limit_min dev.raid.speed_limit_min = 1000 root@vcenter: sysctl dev.raid.speed_limit_max dev.raid.speed_limit_max = 200000 The values can be changed using the systemctl command as well. The numbers are in kilobytes/second on a per-device basis. The following will set a limit of around 7 MB/sec. sudo sysctl -w dev.raid.speed_limit_max=7000 The following are some values based upon the number of drives. The /etc/sysctl.conf file contains the default values for the system. The following is an exmaple of the content. #################NOTE ################ ## You are limited by CPU and memory too # ########################################### dev.raid.speed_limit_min = 50000 ## good for 4-5 disks based array ## dev.raid.speed_limit_max = 2000000 ## good for large 6-12 disks based array ### dev.raid.speed_limit_max = 5000000","title":"Speeds for Check and Rebuild Operations"},{"location":"raid/#linux-unified-key-setup-luks","text":"Another important decision is to whether the data being written to the RAID will be protected through the use of encryption. The use of encryption with the RAID ensures that all data placed on the RAID will not be readable by unauthorized individuals that may have physical access to the drives. Disk encryption within Linux commonly utilizes a combination of dm-crypt and Linux Unified Key Setup (LUKS). The dm-crypt module handles the cryptographic operations while LUKS provides the key management needed for dm-crypt to operate.","title":"Linux Unified Key Setup (LUKS)"},{"location":"raid/#setup_1","text":"Integration with RAID devices configured in previous section would enable the ability to have redundancy and protection of data. Once enabled, a password will need to be provided each time the RAID is mounted. Enabling protection also means that it won't be possible for the system to automatically recover in event of power loss. The following command will initiate the creation process on the /dev/md/critical block device. cryptsetup -y -v luksFormat /dev/md/critical A prompt for password entry will be appear. Be sure to keep the password in safe place as there is no recovery path if forgotten. Once completed, the RAID device will be encrypted but there won't be a file-system present. Also, the encrypted volume will need to be mapped into the file hierarchy before it can be utilized. The following command will mount the encrypted partition to the provided mapper point. In this case, the mapper's name is called secret. cryptsetup luksOpen /dev/md/critical <mapper_point> Ex. cryptsetup luksOpen /dev/md/critical secret By default, the partition creation process does not fill all the drive's contents. This means that any data previously stored on the drive is still present. This can be alleviated by fill the secret partition with random data that covers entire drive. pv -tpreb /dev/zero | dd of=/dev/mapper/secret bs=128M The filling process will take a long time and will vary depending on the speed and size of the drives. Finally, the last step in the setup process is to create a file-system on the RAID device. This will allow for mounting the encryption volume/RAID into the file-system and provide transparent encryption and redundancy in store. The following command will create an ext4 file-system within the encrypted partition. mkfs.ext4 /dev/mapper/secret Once the file-system is created it will need to be mounted into the file hierarchy. mount /dev/mapper/secret /mnt/critical","title":"Setup"},{"location":"raid/#unlocking-process","text":"There are 2 steps associated with unlocking an encrypted partition using LUKS. Encrypted Partition needs to be mounted to an accessible block device. File-system on the device must be mounted to become accessible. The commands seen above show an example of the process but a more targeted combination are provided for preference. # Mount Block Device cryptsetup luksOpen /dev/md/critical secret # Mount file-system mount -t ext4 /dev/mapper/secret /mnt/critical The previous shows the process for making the encrypted partition available for file storage at the /mnt/critical location.","title":"Unlocking Process"},{"location":"raid/#locking-process","text":"The following is the process for locking the RAID devices to prevent data corruption on power down. The following sequence should be followed when shutting down the system to ensure that everything remains in consistent state. # Unmount the file-system to prevent use umount /mnt/critical # Close encrypted partitiont to remove key from memory cryptsetup luksClose secret","title":"Locking Process"},{"location":"raid/#key-backup","text":"The LUKS package utilizes a series of headers to store the decryption keys in a wrapped format. The password that is entered is utilized to derive a key that is utilized to protect the actual encryption key for the partition. This design allows for changing of password without requiring the entire drive to be re-encrypted. The LUKS header can actually maintain several different passwords that will unlock the disk. This is done through a series of slots that can be utilized for storing the wrapped keys. The following command can be utilized to dump the header associated with the LUKS partition. cryptsetup luksDump /dev/mapper/secret","title":"Key Backup"},{"location":"rclone/","text":"rclone rclone is a utility that supports a wide variety of storage services for the movements of data. rclone is design to work in a similar fashion to rsync and support many similar options. The rclone help menu can be opened with the --help option. Additional information on rclone can be found at https://rclone.org/. root@www:/mnt/critical/Secure_maintenance/network-services# rclone --help Rclone syncs files to and from cloud storage providers as well as mounting them, listing them in lots of different ways. See the home page (https://rclone.org/) for installation, usage, documentation, changelog and configuration walkthroughs. Usage: rclone [flags] rclone [command] Available Commands: about Get quota information from the remote. authorize Remote authorization. backend Run a backend-specific command. bisync Perform bidirectional synchronization between two paths. cat Concatenates any files and sends them to stdout. check Checks the files in the source and destination match. checksum Checks the files in the source against a SUM file. cleanup Clean up the remote if possible. completion Generate the autocompletion script for the specified shell config Enter an interactive configuration session. copy Copy files from source to dest, skipping identical files. copyto Copy files from source to dest, skipping identical files. copyurl Copy url content to dest. cryptcheck Cryptcheck checks the integrity of a crypted remote. cryptdecode Cryptdecode returns unencrypted file names. dedupe Interactively find duplicate filenames and delete/rename them. delete Remove the files in path. deletefile Remove a single file from remote. genautocomplete Output completion script for a given shell. gendocs Output markdown docs for rclone to the directory supplied. hashsum Produces a hashsum file for all the objects in the path. help Show help for rclone commands, flags and backends. link Generate public link to file/folder. listremotes List all the remotes in the config file. ls List the objects in the path with size and path. lsd List all directories/containers/buckets in the path. lsf List directories and objects in remote:path formatted for parsing. lsjson List directories and objects in the path in JSON format. lsl List the objects in path with modification time, size and path. md5sum Produces an md5sum file for all the objects in the path. mkdir Make the path if it doesnt already exist. mount Mount the remote as file system on a mountpoint. move Move files from source to dest. moveto Move file or directory from source to dest. ncdu Explore a remote with a text based user interface. obscure Obscure password for use in the rclone config file. purge Remove the path and all of its contents. rc Run a command against a running rclone. rcat Copies standard input to file on remote. rcd Run rclone listening to remote control commands only. rmdir Remove the empty directory at path. rmdirs Remove empty directories under the path. selfupdate Update the rclone binary. serve Serve a remote over a protocol. settier Changes storage class/tier of objects in remote. sha1sum Produces an sha1sum file for all the objects in the path. size Prints the total size and number of objects in remote:path. sync Make source and dest identical, modifying destination only. test Run a test command touch Create new file or change file modification time. tree List the contents of the remote in a tree like fashion. version Show the version number. Use \"rclone [command] --help\" for more information about a command. Use \"rclone help flags\" for to see the global flags. Use \"rclone help backends\" for a list of supported services. Remote Management Within rclone, data can be pushed or pulled from remotes that have been configured within the system. For instance, pushing data into an S3 bucket would require that a dedicated remote be created that points to Amazon AWS S3. List Remotes The currently configured remotes within the system can be seen with the rclone listremotes command. root@www:/mnt/critical/Secure_maintenance/network-services# rclone listremotes iDriveE2NextcloudProd: iDriveE2VaultwardenProd: It's possible to determine where the file storing the configuration is located with the following command. root@www:/mnt/critical/Secure_maintenance/network-services# rclone config file Configuration file is stored at: /root/.config/rclone/rclone.conf Creation Adding remotes allows for data to be pulled or pushed to additional locations. rclone config The above command will open an interactive menu for managing the remotes within the system. For this section, the 'n' option should be selected to add new targets. root@www:/mnt/critical/Secure_maintenance/network-services# rclone config Current remotes: Name Type ==== ==== iDriveE2NextcloudProd s3 iDriveE2VaultwardenProd s3 e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config Backup Rclone can be utilized to push and pull data to the remotes that are configured. The pushing of data into the remote can be utilized in a backup process. For instance, the results of a duplicity could be pushed into a remote S3 bucket to support offsite backup operations. The following is an example of pushing data from a duplicity backup to an iDrive E2 bucket. The appropriate fields will need to be updated as required. rclone copy --progress <source> <remote>:/<bucket name> The credential programmed into the E2 remote must be provided with write access to the desired remote. The previous command will pull the files from the source directory and upload them into the provided bucket. The following is an example related to the backup of Vaultwarden to an E2 bucket called vaultwarden-duplicity-prod. The --progress flag tell rclone to provide performance metrics as the upload process is in progress. rclone copy --progress /mnt/backup/duplicity/vaultwarden iDriveE2VaultwardenProd:/vaultwarden-duplicity-prod As with all backups, it's important to be able to acquire the files in the event a recovery becomes necessary. This is easily done with rclone. The credentials programmed into the remote will need to have read access to the E2 bucket. The following is the pattern for performing a restoration. rclone copy --progress <remote>:/<bucket name>/<folder> <destination> In the previous example, the remote is the first argument in the copy operation. This tells rclone that the data should be downloaded from the remote and stored wtihin the target destination. rclone copy --progress iDriveE2VaultwardenProd:/vaultwarden-duplicity-prod/2022 /mnt/restore/vaultwarden/2022","title":"RClone"},{"location":"rclone/#rclone","text":"rclone is a utility that supports a wide variety of storage services for the movements of data. rclone is design to work in a similar fashion to rsync and support many similar options. The rclone help menu can be opened with the --help option. Additional information on rclone can be found at https://rclone.org/. root@www:/mnt/critical/Secure_maintenance/network-services# rclone --help Rclone syncs files to and from cloud storage providers as well as mounting them, listing them in lots of different ways. See the home page (https://rclone.org/) for installation, usage, documentation, changelog and configuration walkthroughs. Usage: rclone [flags] rclone [command] Available Commands: about Get quota information from the remote. authorize Remote authorization. backend Run a backend-specific command. bisync Perform bidirectional synchronization between two paths. cat Concatenates any files and sends them to stdout. check Checks the files in the source and destination match. checksum Checks the files in the source against a SUM file. cleanup Clean up the remote if possible. completion Generate the autocompletion script for the specified shell config Enter an interactive configuration session. copy Copy files from source to dest, skipping identical files. copyto Copy files from source to dest, skipping identical files. copyurl Copy url content to dest. cryptcheck Cryptcheck checks the integrity of a crypted remote. cryptdecode Cryptdecode returns unencrypted file names. dedupe Interactively find duplicate filenames and delete/rename them. delete Remove the files in path. deletefile Remove a single file from remote. genautocomplete Output completion script for a given shell. gendocs Output markdown docs for rclone to the directory supplied. hashsum Produces a hashsum file for all the objects in the path. help Show help for rclone commands, flags and backends. link Generate public link to file/folder. listremotes List all the remotes in the config file. ls List the objects in the path with size and path. lsd List all directories/containers/buckets in the path. lsf List directories and objects in remote:path formatted for parsing. lsjson List directories and objects in the path in JSON format. lsl List the objects in path with modification time, size and path. md5sum Produces an md5sum file for all the objects in the path. mkdir Make the path if it doesnt already exist. mount Mount the remote as file system on a mountpoint. move Move files from source to dest. moveto Move file or directory from source to dest. ncdu Explore a remote with a text based user interface. obscure Obscure password for use in the rclone config file. purge Remove the path and all of its contents. rc Run a command against a running rclone. rcat Copies standard input to file on remote. rcd Run rclone listening to remote control commands only. rmdir Remove the empty directory at path. rmdirs Remove empty directories under the path. selfupdate Update the rclone binary. serve Serve a remote over a protocol. settier Changes storage class/tier of objects in remote. sha1sum Produces an sha1sum file for all the objects in the path. size Prints the total size and number of objects in remote:path. sync Make source and dest identical, modifying destination only. test Run a test command touch Create new file or change file modification time. tree List the contents of the remote in a tree like fashion. version Show the version number. Use \"rclone [command] --help\" for more information about a command. Use \"rclone help flags\" for to see the global flags. Use \"rclone help backends\" for a list of supported services.","title":"rclone"},{"location":"rclone/#remote-management","text":"Within rclone, data can be pushed or pulled from remotes that have been configured within the system. For instance, pushing data into an S3 bucket would require that a dedicated remote be created that points to Amazon AWS S3.","title":"Remote Management"},{"location":"rclone/#list-remotes","text":"The currently configured remotes within the system can be seen with the rclone listremotes command. root@www:/mnt/critical/Secure_maintenance/network-services# rclone listremotes iDriveE2NextcloudProd: iDriveE2VaultwardenProd: It's possible to determine where the file storing the configuration is located with the following command. root@www:/mnt/critical/Secure_maintenance/network-services# rclone config file Configuration file is stored at: /root/.config/rclone/rclone.conf","title":"List Remotes"},{"location":"rclone/#creation","text":"Adding remotes allows for data to be pulled or pushed to additional locations. rclone config The above command will open an interactive menu for managing the remotes within the system. For this section, the 'n' option should be selected to add new targets. root@www:/mnt/critical/Secure_maintenance/network-services# rclone config Current remotes: Name Type ==== ==== iDriveE2NextcloudProd s3 iDriveE2VaultwardenProd s3 e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config","title":"Creation"},{"location":"rclone/#backup","text":"Rclone can be utilized to push and pull data to the remotes that are configured. The pushing of data into the remote can be utilized in a backup process. For instance, the results of a duplicity could be pushed into a remote S3 bucket to support offsite backup operations. The following is an example of pushing data from a duplicity backup to an iDrive E2 bucket. The appropriate fields will need to be updated as required. rclone copy --progress <source> <remote>:/<bucket name> The credential programmed into the E2 remote must be provided with write access to the desired remote. The previous command will pull the files from the source directory and upload them into the provided bucket. The following is an example related to the backup of Vaultwarden to an E2 bucket called vaultwarden-duplicity-prod. The --progress flag tell rclone to provide performance metrics as the upload process is in progress. rclone copy --progress /mnt/backup/duplicity/vaultwarden iDriveE2VaultwardenProd:/vaultwarden-duplicity-prod As with all backups, it's important to be able to acquire the files in the event a recovery becomes necessary. This is easily done with rclone. The credentials programmed into the remote will need to have read access to the E2 bucket. The following is the pattern for performing a restoration. rclone copy --progress <remote>:/<bucket name>/<folder> <destination> In the previous example, the remote is the first argument in the copy operation. This tells rclone that the data should be downloaded from the remote and stored wtihin the target destination. rclone copy --progress iDriveE2VaultwardenProd:/vaultwarden-duplicity-prod/2022 /mnt/restore/vaultwarden/2022","title":"Backup"}]}